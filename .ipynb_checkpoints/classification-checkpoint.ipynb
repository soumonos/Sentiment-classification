{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Downloads\\\\Sentiment analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarfile.open(\"tp_sentiment_analysis.tar.gz\").extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1: Load and process data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Load the CSV file using the csv import in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ho</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>feedback</th>\n",
       "      <th>label</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1044</td>\n",
       "      <td>125667332931596290</td>\n",
       "      <td>2011-10-16 20:20:01</td>\n",
       "      <td>&amp;quot;3 principal global players will be activ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>126384526925639681</td>\n",
       "      <td>2011-10-18 19:49:53</td>\n",
       "      <td>If you've been struggling to get hold of me, I...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278</td>\n",
       "      <td>126281019476291585</td>\n",
       "      <td>2011-10-18 12:58:35</td>\n",
       "      <td>@azee1v1 @apple @umber Proper consolidation, p...</td>\n",
       "      <td>negative</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5743</td>\n",
       "      <td>126862268725080065</td>\n",
       "      <td>2011-10-20 03:28:16</td>\n",
       "      <td>me acabo de dar cuenta q tengo mas seguidores ...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510</td>\n",
       "      <td>126054725727698944</td>\n",
       "      <td>2011-10-17 21:59:23</td>\n",
       "      <td>With Siri, Apple Could Eventually Build A Real...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                  ho            timestamp  \\\n",
       "0  1044  125667332931596290  2011-10-16 20:20:01   \n",
       "1    71  126384526925639681  2011-10-18 19:49:53   \n",
       "2   278  126281019476291585  2011-10-18 12:58:35   \n",
       "3  5743  126862268725080065  2011-10-20 03:28:16   \n",
       "4   510  126054725727698944  2011-10-17 21:59:23   \n",
       "\n",
       "                                            feedback       label  company  \n",
       "0  &quot;3 principal global players will be activ...     neutral    apple  \n",
       "1  If you've been struggling to get hold of me, I...     neutral    apple  \n",
       "2  @azee1v1 @apple @umber Proper consolidation, p...    negative    apple  \n",
       "3  me acabo de dar cuenta q tengo mas seguidores ...  irrelevant  twitter  \n",
       "4  With Siri, Apple Could Eventually Build A Real...     neutral    apple  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_list= [\"id\",\"ho\",\"timestamp\",\"feedback\",\"label\",\"company\"]\n",
    "df_raw= pd.read_csv(\"sanders-twitter-sentiment.csv\",names=header_list)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2:Produce a python list of pre-processed tweets using the script preprocess_twitter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_feedback= [preprocess_twitter.preprocess(i) for i in df_raw.feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_labels= [preprocess_twitter.preprocess(i) for i in df_raw.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"if you've been struggling to get hold of me, i'm back online with a new iphone - thanks <user>\",\n",
       " '<user> <user> <user> proper consolidation, proper syncing, stop losing my purchased items, checkboxes that do what you think they will do.',\n",
       " 'me acabo de dar cuenta q tengo mas seguidores en <hashtag>  twitter <smile> gracias y espero q sigan sumandoo mas y mas <smile> para seguir aun mas activo aki',\n",
       " 'with siri, apple could eventually build a real ai <url>  + <user> <hashtag>  apple <hashtag>  siri',\n",
       " 'all this talk about about how good <hashtag> icecreamsandwich looks is making me hungry <hashtag> google <hashtag> omnomnomnom',\n",
       " '<hashtag>  galaxy nexus es presentado oficialmente. <url> | conoce los detalles del nuevo telã©fono de <hashtag>  google',\n",
       " \"rt <user>: <hashtag> twitter; for those statuses you'd never post on <hashtag> facebook\",\n",
       " 'one serious statement tho !  <hashtag>  twitter blocked the <hashtag>  troy davis <hashtag> tt <allcaps>  <hashtag>  twitter was cool w <hashtag>  amber cole  in my opinion . <repeat> bullshit .',\n",
       " '<hashtag>  microsoft ofrece un sistema de <hashtag> codificaciã³n <hashtag>  microsoft tag <url>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_feedback[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'negative',\n",
       " 'irrelevant',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'irrelevant',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'irrelevant']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_labels[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Use these python lines to process data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text,ml):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    data = pad_sequences(sequences, maxlen=ml)\n",
    "    vocab = tokenizer.word_index\n",
    "    vocab['<eos>'] = 0\n",
    "    return data,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_vectorized,vocabfv= vectorize(preprocessed_feedback,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_vectorized,vocablv= vectorize(preprocessed_labels,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5513, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5513, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Now you should have two numpy arrays which shape are (5513, 32) and (5513, 4) for tweets and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= feedback_vectorized[:4000]\n",
    "x_test= feedback_vectorized[4000:]\n",
    "y_train= labels_vectorized[:4000]\n",
    "y_test= labels_vectorized[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrains, xvals,ytrains,yval_s= train_test_split(feedback_vectorized,labels_vectorized, test_size=0.275,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"glove.twitter.27B.100d.filtered.txt\"\n"
     ]
    }
   ],
   "source": [
    "weights_feedbacks= embedding1.load(vocabfv,100,'glove.twitter.27B.100d.filtered.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"glove.twitter.27B.100d.filtered.txt\"\n"
     ]
    }
   ],
   "source": [
    "weights_labels= embedding1.load(vocablv,100,'glove.twitter.27B.100d.filtered.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, GRU, Dense\n",
    "from keras.models import Model\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall_m(yval_s, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(yval_s * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(yval_s, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(yval_s, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(yval_s * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(yval_s, y_pred):\n",
    "    precision = precision_m(yval_s, y_pred)\n",
    "    recall = recall_m(yval_s, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "3996/3996 [==============================] - 4s 1ms/step - loss: 0.1118 - acc: 0.9950 - f1_m: 1.0018 - precision_m: 1.0721 - recall_m: 0.9886\n",
      "Epoch 2/10\n",
      "3996/3996 [==============================] - 3s 835us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 3/10\n",
      "3996/3996 [==============================] - 3s 810us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 4/10\n",
      "3996/3996 [==============================] - 3s 863us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 5/10\n",
      "3996/3996 [==============================] - 3s 876us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 6/10\n",
      "3996/3996 [==============================] - 4s 910us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 7/10\n",
      "3996/3996 [==============================] - 4s 896us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 8/10\n",
      "3996/3996 [==============================] - 4s 894us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 9/10\n",
      "3996/3996 [==============================] - 4s 891us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 10/10\n",
      "3996/3996 [==============================] - 4s 896us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(vocabfv), 100, weights=[weights_feedbacks],input_length=32, trainable=False)\n",
    "sequence_input = Input(shape=(32,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = GRU(64)(embedded_sequences)\n",
    "preds = Dense(labels_vectorized.shape[1], activation='softmax')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Nadam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "trained= model.fit(xtrains, ytrains,epochs=10, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 4s 929us/step - loss: 0.1736 - acc: 0.9827 - f1_m: 0.9992 - precision_m: 3968254.0000 - recall_m: 0.9752\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 3s 776us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 3s 776us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 3s 789us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 3s 796us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 3s 789us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 3s 800us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 3s 783us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 3s 829us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 3s 797us/step - loss: 0.0000e+00 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(vocabfv), 100, weights=[weights_feedbacks],input_length=32, trainable=False)\n",
    "sequence_input = Input(shape=(32,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = GRU(64)(embedded_sequences)\n",
    "preds = Dense(labels_vectorized.shape[1], activation='softmax')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Nadam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "trained= model.fit(x_train, y_train,epochs=10, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_crossentropy_lossr, accuracyr, f1_scorer, precisionr, recallr = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_crossentropy_lossr, accuracyr, f1_scorer, precisionr, recallr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(32,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(3)(x) # global max pooling\n",
    "x = Flatten()(x)\n",
    "preds = Dense(labels_vectorized.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Nadam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "trained= model.fit(x_train, y_train,epochs=10, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_crossentropy_lossc, accuracyc, f1_scorec, precisionc, recallc = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_crossentropy_lossc, accuracyc, f1_scorec, precisionc, recallc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
